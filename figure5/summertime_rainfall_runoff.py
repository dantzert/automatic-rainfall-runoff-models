# -*- coding: utf-8 -*-
"""summertime-rainfall-runoff.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lSzy6aoX2F7Nd2wgGFaG8BLLHS5Y24oJ

# imports
"""

# imports and config
import pandas as pd
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
#!pip3 install pysindy
import pysindy as ps
import scipy.stats as stats
from scipy import signal
import requests
import json
from matplotlib.gridspec import GridSpec

"""# grab data from usgs rest api"""

grand_rapids_mi_id = "04119000"
croton_mi_id = "04121970"
palmer_mi_id = "04058200"
sidnaw_mi_id = "04040500"
newburg_mo_id = "06932000"
show_low_az_id = "09496500"
sedona_az_id = "09504420"
valle_az_id = "09404104"
bisbee_az_id = "09470700"
myakka_head_fl_id = "02299950"
fort_meade_fl_id = "02294898"

# handpicked for small contributing area
boxelder_creek_nemo_sd = "06422500"
grizzly_bear_creek_keystone_sd = "06403850" # (only has stage, not discharge)
grace_coolidge_creek_custer_sd = "06404998"
boggy_creek_enid_ok = "07160280" # only stage, no discharge
north_boggy_creek_ok = "07160290" # only stage no discharge
skeleton_creek_ok = "07160350"
rio_ruidoso_nm = "08386505"

mill_creek_ok = "07331200"
# olmos and salado could have longer records
olmos_ck_tx = "08177700"
salado_ck_tx = "08178700"
san_pedro_ck_tx = "08178500"
big_fossil_ck_tx = "08048800"
walnut_ck_tx = "08049700"
langham_ck = "08072760"
bundick = "08014800"
baker_ck = "02303205"
# below are from around tampa bay fl
braden_riv = "02300033"
north_archie = "02301740"
st_joe_ck = "02308950"
long_branch = "02307780"
alligator_ck = "02307674"

sites = [mill_creek_ok, olmos_ck_tx, salado_ck_tx, san_pedro_ck_tx, big_fossil_ck_tx,
         walnut_ck_tx, langham_ck, bundick, baker_ck, braden_riv, north_archie, st_joe_ck,
         long_branch, alligator_ck, newburg_mo_id, myakka_head_fl_id, fort_meade_fl_id, 
         skeleton_creek_ok,show_low_az_id, north_boggy_creek_ok, rio_ruidoso_nm]


# updating label sizes for paper submission
sites = [long_branch, north_archie, rio_ruidoso_nm]

# some sites (sidnaw) have had equipment failures which causes errors in parsing the data

start_date = "2022-07-15" # fallback dates
end_date = "2022-11-30"

# parameters = "00045,00060,00065" # rainfall, discharge, and gage height
site_data = dict.fromkeys(sites)
for site_id in sites:
  if (site_id == newburg_mo_id 
      or site_id == skeleton_creek_ok 
      or site_id == rio_ruidoso_nm
      or site_id == show_low_az_id
      or site_id == north_boggy_creek_ok):
      start_date = "2022-08-01"

  if (site_id == myakka_head_fl_id 
      or site_id == fort_meade_fl_id
      or site_id == olmos_ck_tx
      or site_id == salado_ck_tx
      or site_id == braden_riv
      or site_id == north_archie
      or site_id == st_joe_ck
      or site_id == alligator_ck):
      start_date = "2021-12-01"


  if (site_id == long_branch):
    start_date = "2022-02-15"
  if (site_id == baker_ck):
    start_date = "2022-02-01"

  if (site_id == mill_creek_ok
      or site_id == san_pedro_ck_tx
      or site_id == big_fossil_ck_tx
      or site_id == walnut_ck_tx
      or site_id == langham_ck
      or site_id == bundick):
      start_date = "2022-08-10"

  request_string = str("https://waterservices.usgs.gov/nwis/iv/?format=rdb,1.0&sites="+site_id+"&startDT="+start_date+"&endDT="+end_date+"&parameterCd=00045,00065&siteStatus=all")
  print(request_string)
  meta = pd.read_csv(request_string, skiprows=14,nrows=10,sep='\t')
  site_name = meta.columns[0][5:]

  data = pd.read_csv(request_string,header = [0,1],sep='\t',comment = '#').fillna(method='bfill')
  if (site_id == rio_ruidoso_nm): # rio ruidoso is weird for some reason
    data = pd.read_csv(request_string,header = [0,1],sep='\t',comment = '#').fillna(method='pad')
  #print(data)


  #for_sindy = data.to_numpy()[:,[4,6,8]].astype(float)
  for_sindy = data.to_numpy()[:,[4,6]].astype(float)
  #for_sindy[:,1] = for_sindy[:,1] - min(for_sindy[:,1])

  for_sindy[:,1] = for_sindy[:,1] - np.min(for_sindy[:,1])

  #discharge = signal.savgol_filter(for_sindy[:,1],window_length=5,polyorder=1)
  # use moving average data for sindy
  #for_sindy[:,1] = discharge
  for_sindy[:,1] = signal.savgol_filter(for_sindy[:,1],window_length=5,polyorder=1)
  site_data[site_id] =[site_name, data,for_sindy]
  '''
  fig, ax1 = plt.subplots(figsize=(20,10))
  plt.title(site_name)
  ax2 = ax1.twinx()
  l1 = ax1.plot(for_sindy[:,0],'b',label='precip')
  l2 = ax2.plot(for_sindy[:,1],'r',label='discharge')
  ax1.set_ylabel("precip (in)")
  ax2.set_ylabel("discharge (cfs)")
  lns = l1+l2
  labs = [l.get_label() for l in lns]
  plt.legend(lns, labs, loc=0)

  plt.show()
  '''

#print(site_data)

"""# fitting functions definition"""

def SINDY_delays_3d(shape_factors, scale_factors, loc_factors, t, forcing, response, final_run, poly_degree):
  shape_time = np.arange(0,len(t),1) # analogous to drainage_time
  # shapes is analogous to "drainage" in original code
  feature_names=['response']# , 'forcing']
  shapes = np.zeros(shape=(len(t), len(shape_factors)))
  for shape_idx in range(0,len(shape_factors)):
    for idx in range(0,len(t)):
      if (abs(forcing[idx]) > 10**-6): # when nonzero forcing occurs
        if (idx == int(0)):

          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time, shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 

        else:
          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time[:-idx], shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 


    feature_names.append(str("forcing" + str(shape_idx + 1)) )
  
  # SINDy
  model = ps.SINDy(
      differentiation_method= ps.SmoothedFiniteDifference(),
      feature_library=ps.PolynomialLibrary(degree=poly_degree,include_bias = False, include_interaction=False), 
      optimizer = ps.STLSQ(threshold=0), 
      feature_names = feature_names
  )

  #U = np.concatenate((np.reshape(forcing,(-1,1)), shapes) , axis=1)
  #if (any(shapes == np.inf)):
  #  print("infinite value in U")
  #U = np.nan_to_num(shapes,nan=0.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)
  U = shapes
  model.fit(response,t=t,u=U)
  #model.print()
  #print("score = ",model.score(response,t=t,u=U)) # training data score

  mae = 10**6 # placeholder, shows simulation diverged or wasn't final run
  rmse = 10**6
  simulated = np.ones(shape=(len(response[1:]),1))*np.mean(response)
  if (final_run): 
    '''
    print("number of lines:")
    print(len(shape_factors))
    plt.figure(figsize=(20,10))
    plt.plot(U)
    plt.title("final input transformations")
    plt.show()
    
    plt.figure(figsize=(5,5))
    plt.title("input transformation parameters")
    plt.scatter(loc_factors, shape_factors)
    plt.xlabel("location")
    plt.ylabel("shape")
    plt.show()
    '''

    model.print(precision=5)
    print("score = ",model.score(response,t=t,u=U)) # training data score

    try: # in case simulation diverges
      y = np.reshape(response,(-1,1)) # to match simulation results dimension
      simulated = model.simulate([response[1]],t=t,u=U)
      '''
      fig, ax1 = plt.subplots(figsize=(20,10))
      ax2 = ax1.twinx()
      plt.title(str("polynomial degree = "+ str(poly_degree)))

      line1 = ax1.plot(t[1:],y[1:],'r-.',label='measurements')
      line2 = ax1.plot(t[1:],simulated[:len(t)-1],'g--',label='SINDy')
      ax1.set_xlabel(r'Time ($min$)')
      ax1.set_ylabel(r'response magnitude')
      ax1.set_ylim([min(0,min(y)),1.2*max(y)])

      line3 = ax2.plot(t[1:],forcing[1:],'b',label='forcing')
      ax2.set_ylabel("forcing intensity")

      line4 = ax2.plot(t[1:], shapes[1:] * ( np.max(forcing) / np.max(shapes  ) ) , 'k--', label='transformed forcing (scaled)', alpha=0.35)
      
      lns = line1+line2+line3+line4
      labs = [l.get_label() for l in lns]
      plt.legend(lns, labs, loc=0,ncol=3)


      plt.show()
      '''
      mae = np.mean(np.abs(simulated[:len(t)-1]-y[1:])) # mean absolute error
      print("Simulation MAE = ", mae)
      rmse = np.sqrt(np.mean((simulated[:len(t)-1]-y[1:])**2)) # root mean squared error
      print("Simulation RMSE = ", rmse)


    except:
      print("Simulation diverged.")


    


  return [model.score(response,t=t,u=U), model, mae, rmse, t[1:], simulated[:len(t)-1] , response[1:] , forcing[1:] , U]

# takes np array X and assumes the zeroeth column is the forcing
def optimize_lag_shapes(polyorder, target, init_num_lines, max_num_lines, X,max_iter):
  results = list()

  rates_of_change = abs(np.diff(X[:,target]))
  biggest_movers = np.flip(np.argsort(rates_of_change))
  maxes = np.array([biggest_movers[0]])
  if (maxes[0] < 1):
    maxes[0] = 1 # lower bound shape at 1

  # if a close neighbor is already selected, don't want that to be a starting point
  for index in range(0,len(biggest_movers)):
    if ( (abs(biggest_movers[index] - maxes) > int(len(X) / 1000*init_num_lines)).all()   ): 
      # more than some fraction of the total length apart
      # using init_num_lines strikes a balance between starting evenly spaced 
      # and starting all clustered on the steep regions
      if (biggest_movers[index] < 1):
        maxes = np.append(maxes,1) 
      else:
        maxes = np.append(maxes,float(biggest_movers[index]))
    if (len(maxes) >= max_num_lines):
      break

  previous_best = 0

  shape_factors = np.array([])
  scale_factors = np.array([])
  loc_factors = np.array([])
  speeds =  list([1000,500,200,100,50,10, 5,2, 1.1, 1.05, 1.01, 1.001])
  
  for num_lines in range(init_num_lines,max_num_lines):
    #print(num_lines)
    #speed = 6.4 # how far we'll jump around initially
    speed_idx = 0
    speed = speeds[speed_idx]

    if (len(shape_factors) == 0):  # if we're starting right now
      
      #shape_factors = maxes[0:init_num_lines] (this is causing divergence sometimes I think)
      # the above is a really good strategy if we have forcing at time zero, but can cause issues if there's no rainfall for a while at the beginning
      shape_factors = np.ones(init_num_lines)
      # start assuming immediate impact
      #shape_factors = np.ones(shape=(init_num_lines,1))
      scale_factors = np.ones(shape = shape_factors.shape)
      loc_factors = np.zeros(shape = shape_factors.shape)

      
    else:
      # start dull
      sharp_cand = maxes[num_lines]
      # start assuming immeidate impact
      #sharp_cand = 1
      delay_cand = 1
      loc_cand = 0

      shape_factors = np.append(shape_factors, sharp_cand)
      scale_factors = np.append(scale_factors, delay_cand)
      loc_factors = np.append(loc_factors, loc_cand)

    # changed prev model to true for verbose output
    prev_model = SINDY_delays_3d(shape_factors, scale_factors, loc_factors, np.arange(0,len(X)), X[:,0], X[:,target],False, polyorder )

    print("\nInitial model:\n")
    print("score")
    print(prev_model[0])
    print("shape factors")
    print(shape_factors)
    print("scale factors")
    print(scale_factors)
    print("location factors")
    print(loc_factors)
    print("")

    for iterations in range(0,max_iter ):
      tuning_line = iterations % num_lines

      sooner_locs = np.array(loc_factors)
      sooner_locs[tuning_line-1] = float(loc_factors[tuning_line-1] - speed*len(X)/10**4  )
      if ( sooner_locs[tuning_line-1] < 0):
        sooner = np.zeros(len(prev_model))
      else:
        sooner = SINDY_delays_3d(shape_factors ,scale_factors ,sooner_locs, 
          np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      
      
      later_locs = np.array(loc_factors)
      later_locs[tuning_line-1] = float ( loc_factors[tuning_line-1]  +   1.01*speed*len(X)/10**4 )
      later = SINDY_delays_3d(shape_factors , scale_factors,later_locs, 
          np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      

      shape_up = np.array(shape_factors)
      shape_up[tuning_line-1] = float ( shape_factors[tuning_line-1]*speed*1.01 )
      shape_upped = SINDY_delays_3d(shape_up , scale_factors, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      
      shape_down = np.array(shape_factors)
      shape_down[tuning_line-1] = float ( shape_factors[tuning_line-1]/speed )
      if (shape_down[tuning_line-1] < 1):
        shape_downed = np.zeros(len(prev_model)) # return a score of zero as this is illegal
      else:
        shape_downed = SINDY_delays_3d(shape_down , scale_factors, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )

      scale_up = np.array(scale_factors)
      scale_up[tuning_line-1] = float(  scale_factors[tuning_line-1]*speed*1.01 )
      scaled_up = SINDY_delays_3d(shape_factors , scale_up, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )


      scale_down = np.array(scale_factors)
      scale_down[tuning_line-1] = float ( scale_factors[tuning_line-1]/speed )
      scaled_down = SINDY_delays_3d(shape_factors , scale_down, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      
      # rounder
      rounder_shape = np.array(shape_factors)
      rounder_shape[tuning_line-1] = shape_factors[tuning_line-1]*(speed*1.01)
      rounder_scale = np.array(scale_factors)
      rounder_scale[tuning_line-1] = scale_factors[tuning_line-1]/(speed*1.01)
      rounder = SINDY_delays_3d(rounder_shape , rounder_scale, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )

      # sharper
      sharper_shape = np.array(shape_factors)
      sharper_shape[tuning_line-1] = shape_factors[tuning_line-1]/speed
      if (sharper_shape[tuning_line -1] < 1):
        sharper = np.zeros(len(prev_model)) # lower bound on shape to avoid inf
      else:
        sharper_scale = np.array(scale_factors)
        sharper_scale[tuning_line-1] = scale_factors[tuning_line-1]*speed
        sharper = SINDY_delays_3d(sharper_shape ,sharper_scale,loc_factors, 
                                  np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )


    

      scores = [prev_model[0], shape_upped[0], shape_downed[0], scaled_up[0], scaled_down[0], sooner[0], later[0], rounder[0], sharper[0] ]
      #print(scores)
      if (sooner[0] >= max(scores)):
        prev_model = sooner
        loc_factors = sooner_locs
      elif (later[0] >= max(scores)):
        prev_model = later
        loc_factors = later_locs

      elif(shape_upped[0] >= max(scores)):
        prev_model = shape_upped
        shape_factors = shape_up
      elif(shape_downed[0] >=max(scores)):
        prev_model = shape_downed
        shape_factors = shape_down

      elif(scaled_up[0] >= max(scores)):
        prev_model = scaled_up
        scale_factors = scale_up
      elif(scaled_down[0] >= max(scores)):
        prev_model = scaled_down
        scale_factors = scale_down


      elif (rounder[0] >= max(scores)):
        prev_model = rounder
        shape_factors = np.array(rounder_shape)
        scale_factors = np.array(rounder_scale)
      elif (sharper[0] >= max(scores)):
        prev_model = sharper
        shape_factors = np.array(sharper_shape)
        scale_factors = np.array(sharper_scale)


      elif( (num_lines - 1) == tuning_line): # the middle was best, but it's bad, tighten up the bounds (if we're at the last tuning line)
        
        speed_idx = speed_idx + 1
        
        if (speed_idx >= len(speeds)):
          break
        speed = speeds[speed_idx]
        
        '''
        if (speed > 10):
          speed = speed*0.5 # coarse tuning
          
        elif(speed > 1.3):
          speed = speed*0.8 # fine tuning
        else:
          speed = speed*0.95 # very fine tuning
        '''
        '''
        print("\nprevious, shape up, shape down, scale up, scale down, sooner, later, rounder, sharper")
        print(scores)
        print("speed")
        print(speed)
        print("shape factors")
        print(shape_factors)
        print("scale factors")
        print(scale_factors)
        print("location factors")
        print(loc_factors)
        print("iteration no:")
        print(iterations)
        print("\n")
        '''
        #print(scores)
        if (speed < 1):
          print("converged, max accuracy for number of lines")
          break
    
    
    final_model = SINDY_delays_3d(shape_factors, scale_factors ,loc_factors,np.arange(0,len(X)), X[:,0], X[:,target], True, polyorder )
    results.append([final_model, shape_factors, scale_factors, loc_factors])
    if ((previous_best and final_model[0] - previous_best[0] < 0.005) or ( previous_best and final_model[0] - previous_best[0] < 0.01 and final_model[2] > previous_best[2] )): 
      # marginal (less than half percent improvement) OR somewhat marginal improvement in fit and worse mae
      break # we can be done
    else:
      previous_best = final_model
  winning_model = None
  best_mae = np.Inf
  for num_lines in range(len(results)):
    if (results[num_lines][0][2] < best_mae):
      winning_model = results[num_lines]
      best_mae = results[num_lines][0][2]

  return winning_model
    
def transform_input(shape_factors, scale_factors, loc_factors,t, forcing):
  shape_time = np.arange(0,len(t),1) # analogous to drainage_time
  # shapes is analogous to "drainage" in original code
  feature_names=['response']# , 'forcing']
  shapes = np.zeros(shape=(len(t), len(shape_factors)))
  for shape_idx in range(0,len(shape_factors)):
    for idx in range(0,len(t)):
      if (abs(forcing[idx]) > 10**-6): # when nonzero forcing occurs
        if (idx == int(0)):

          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time, shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 

        else:
          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time[:-idx], shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 

  return shapes

"""# training"""

init_num_lines = 1
max_num_lines = 6
max_polyorder = 3
target = 1 # column of x to target, 0 is rainfall
max_iterations = 200

for site in site_data:

  '''
  #print(site_data[site][2])
  #print(type(site_data[site][2][0,1]))
  #print(sum(sum(np.isnan(site_data[site][2]))))
  '''


  x_train = np.array(site_data[site][2][:int(0.90*len(site_data[site][1]))]) # first 90% of the record is for training
  x_test = np.array(site_data[site][2][int(0.90*len(site_data[site][1])):]) # the rest is for testing


  '''
  # smooth the discharge and height measurements (only for training, doesn't matter for testing)
  #x_train[:,1] = signal.savgol_filter(x_train[:,1],window_length=5,polyorder=3)
  #x_train[:,2] = signal.savgol_filter(x_train[:,2],window_length=5,polyorder=2)
  '''


  print(str("starting for " + str(site_data[site][0])))
  plt.close('all')
  simulations = list()
  mae = list()
  training_simulations = list()
  training_mae = list()
  r2scores=list()
  for polyorder in range(1, max_polyorder+1):
      print(str("order: "+ str(polyorder) ) )
      best_model = optimize_lag_shapes(polyorder, target, init_num_lines, max_num_lines, x_train, max_iterations)
      r2scores.append(best_model[0][0])
      U = transform_input(best_model[1], best_model[2], best_model[3], np.arange(len(x_test)) , x_test[:,0]) 
      try:
          simulations.append(best_model[0][1].simulate([x_test[0,target]] , np.arange(len(x_test)), u=U))
          mae.append(np.mean(np.abs(simulations[polyorder-1]-x_test[1:,target])))
      except:
          simulations.append(np.ones(len(x_test[1:,target])) * np.mean(x_test[1:,target]) )
          mae.append(10**6)
          #print(str("target: " + str( site_data[site][0] ) + "  |  polyorder: " + str(polyorder)))
          print("simulation diverged")


        # simulate response to training storm and compare to actual
      U_train = transform_input(best_model[1], best_model[2], best_model[3], np.arange(len(x_train)) , x_train[:,0]) 
      try:
          training_simulations.append(best_model[0][1].simulate([x_train[0,target]] , np.arange(len(x_train)) , u = U_train ) )
          training_mae.append(np.mean(np.abs(training_simulations[polyorder-1]-x_train[1:,target])))
      except:
          training_simulations.append(np.ones(len(x_train[1:,target])) * np.mean(x_train[1:,target]) )
          training_mae.append(10**6)
          #print(str("target: " + str( site_data[site][0] ) + "  |  polyorder: " + str(polyorder)))
          print("training simulation diverged")
  
  # plot the model with the best training R^2 and mae
  # if those measures differ, use the lower order one
  print("r2 socres")
  print(r2scores)
  print("training mae")
  print(training_mae)
  print("testing mae")
  print(mae)

  best_r2 = np.argmax(r2scores)
  best_mae = np.argmin(training_mae)
  print("best r2 index")
  print(best_r2)
  print("best mae index")
  print(best_mae)
  # plot the model with the best training mae
  to_plot_idx = best_mae#min(best_r2, best_mae) 

  print("plotting model: ")
  print(to_plot_idx)

  train_days = np.arange(0,len(x_train[1:,target]))*15/24/60
   # visualize training results
  fig = plt.figure(figsize=(25,10))
  gs = GridSpec(2,1, figure=fig, height_ratios=[1,4])
  
  ax2 = fig.add_subplot(gs[0,0])
  ax2.invert_yaxis()
  ax2.tick_params(axis='x',labelbottom=False,which='both',bottom=False)
  ax2.tick_params(axis='y',labelsize=20)
  l5 = ax2.plot(train_days, x_train[1:,0], 'b',label='Rainfall',alpha=1,linewidth=5)

  ax2.legend(["Rainfall [in/hr]"],fontsize=45,loc=0)
  ax2.spines['right'].set_visible(False)
  ax2.spines['bottom'].set_visible(False)


  ax1 = fig.add_subplot(gs[1,0])
  l1 = ax1.plot(train_days,x_train[1:,target],'k',label='Measured Stage',alpha=0.5,linewidth=5)
  l2 = ax1.plot(train_days,training_simulations[to_plot_idx],'r--',label=str("Best Fit" ),alpha=1,linewidth=5)

  ax1.set_xlabel(r'Time [days]',fontsize=35)
  ax1.set_ylabel(r'Stage - min(Stage) [ft]',fontsize=35)
  #ax1.set_ylim([min(x_train[1:,target] ),1.2*max(x_train[1:,target])])
  ax1.spines['top'].set_visible(False)
  ax1.spines['right'].set_visible(False)
  ax1.tick_params(axis='both',labelsize=20)
  lns = l1+l2

  labs = [l.get_label() for l in lns]
  ax1.legend(lns, labs, loc=0,fontsize=45)

  
  fig.suptitle(str("Training "+ str(str(site_data[site][0])  + " | R^2 = {r2:.2f}".format(r2=r2scores[to_plot_idx])  )  ) ,fontsize=35,y=0.98)
  plt.tight_layout()
  plt.savefig(str("G:/My Drive/SINDy/usgs/usgs_results/" + str( site_data[site][0] ) + "_training.png"), dpi=300)
  plt.close()


          
  test_days = np.arange(0,len(x_test[1:,target]))*15/24/60        
  # visualize testing results
  fig= plt.figure(figsize=(25,10))
  gs = GridSpec(2,1,figure=fig,height_ratios=[1,4])
  

  ax2 = fig.add_subplot(gs[0,0])
  ax2.invert_yaxis()
  ax2.tick_params(axis='x',labelbottom=False,which='both',bottom=False)
  ax2.tick_params(axis='both',labelsize='xx-large')
  l5 = ax2.plot(test_days,x_test[1:,0], 'b',label='Rainfall',alpha=1,linewidth=5)
  ax2.legend(["Rainfall [in/hr]"],fontsize=45,loc=0)
  ax2.spines['right'].set_visible(False)
  ax2.spines['bottom'].set_visible(False)
  

  ax1 = fig.add_subplot(gs[1,0])
  l1 = ax1.plot(test_days,x_test[1:,target],'k',label='Measured Stage',alpha=0.5,linewidth=5)
  l2 = ax1.plot(test_days,simulations[to_plot_idx],'g--',label=str("Prediction"),linewidth=5,alpha=1)
  ax1.tick_params(axis='both',labelsize='xx-large')
  ax1.set_xlabel(r'Time [days]',fontsize=35)
  ax1.set_ylabel(r'Stage - min(Stage) [ft]',fontsize=35)
  ax1.spines['top'].set_visible(False)
  ax1.spines['right'].set_visible(False)
  ax1.tick_params(axis='both',labelsize=20)
  lns = l1+l2

  labs = [l.get_label() for l in lns]
  ax1.legend(lns, labs, loc=0,fontsize=45)

  fig.suptitle(str("Testing "+ str(site_data[site][0]) + " | MAE = {mae:.2f} ft".format(mae=mae[to_plot_idx]) ),fontsize=35,y=0.98)
  
  plt.tight_layout()
  plt.savefig(str("G:/My Drive/SINDy/usgs/usgs_results/" + str( site_data[site][0] ) + "_testing.png"), dpi=200)
  plt.close()
  
  

  
  # training and testing together
  fig = plt.figure(figsize=(40,10))
  gs = GridSpec(2,1, figure=fig, height_ratios=[1,4])

  ax2 = fig.add_subplot(gs[0,0])
  ax2.invert_yaxis()
  ax2.tick_params(axis='x',labelbottom=False,which='both',bottom=False)
  ax2.tick_params(axis='both',labelsize='xx-large')
  # training
  ax2.plot(train_days, x_train[1:,0], 'b',label='Rainfall',alpha=1,linewidth=5)
  ax2.legend(["Rainfall [in/hr]"],fontsize=45,loc=0)
  ax2.spines['right'].set_visible(False)
  ax2.spines['bottom'].set_visible(False)

  # testing
  ax2.plot(test_days + train_days[-1],x_test[1:,0], 'b',label='Rainfall',alpha=1,linewidth=5)


  ax1 = fig.add_subplot(gs[1,0])
  l1 = ax1.plot(train_days,x_train[1:,target],'k',label='Measured Stage',alpha=0.5,linewidth=5)
  # continue measured stage
  ax1.plot(test_days + train_days[-1], x_test[1:,target], 'k', alpha=0.5, linewidth=5)
  # best fit
  l2 = ax1.plot(train_days,training_simulations[to_plot_idx],'r--',label=str("Best Fit" ),alpha=1,linewidth=5)
  # prediction
  l3 = ax1.plot(test_days + train_days[-1],simulations[to_plot_idx],'g--',label=str("Prediction"),linewidth=5,alpha=1)
  
  # separate training from prediction periods
  ax1.vlines(train_days[-1], 0, max(x_train[1:,target] ) , color='k', linestyle='--', linewidth=5)
  
  ax1.tick_params(axis='both',labelsize='xx-large')
  ax1.set_xlabel(r'Time [days]',fontsize=35)
  ax1.set_ylabel(r'Stage - min(Stage) [ft]',fontsize=35)
  #ax1.set_ylim([min(x_train[1:,target] ),1.2*max(x_train[1:,target])])
  ax1.spines['top'].set_visible(False)
  ax1.spines['right'].set_visible(False)

  lns = l1+l2 + l3

  labs = [l.get_label() for l in lns]
  ax1.legend(lns, labs, loc=0,fontsize=45)

  fig.suptitle(str(str(site_data[site][0]) + " | R^2 = {r2:.2f} | MAE = {mae:.2f} ft".format(r2=r2scores[to_plot_idx],mae=mae[to_plot_idx]) ),fontsize=45,y=0.98)
  
  plt.tight_layout()
  plt.savefig(str("G:/My Drive/SINDy/usgs/usgs_results/" + str( site_data[site][0] ) + "_combined.png"), dpi=200)
  plt.close()

