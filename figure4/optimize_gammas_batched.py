# -*- coding: utf-8 -*-
"""optimize-gammas-batched.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dMcrlBmL4EgmZGF2lSwmn659IQWNTbxq
"""

# imports and config
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#!pip3 install pysindy
import pysindy as ps
import scipy.stats as stats
#from google.colab import drive

#!pip3 install pyswmm
import pyswmm 
#!pip3 install swmm
import swmm
import datetime
from matplotlib.gridspec import GridSpec



#drive.mount('/content/drive',force_remount=True)

"""# fitting functions"""

def SINDY_delays_3d(shape_factors, scale_factors, loc_factors, t, forcing, response, final_run, poly_degree):
  shape_time = np.arange(0,len(t),1) # analogous to drainage_time
  # shapes is analogous to "drainage" in original code
  feature_names=['response']# , 'forcing']
  shapes = np.zeros(shape=(len(t), len(shape_factors)))
  for shape_idx in range(0,len(shape_factors)):
    for idx in range(0,len(t)):
      if (abs(forcing[idx]) > 10**-6): # when nonzero forcing occurs
        if (idx == int(0)):

          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time, shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 

        else:
          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time[:-idx], shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 


    feature_names.append(str("forcing" + str(shape_idx + 1)) )
  
  # SINDy
  model = ps.SINDy(
      differentiation_method= ps.SmoothedFiniteDifference(),
      feature_library=ps.PolynomialLibrary(degree=poly_degree,include_bias = False, include_interaction=False), 
      optimizer = ps.STLSQ(threshold=0), 
      feature_names = feature_names
  )

  #U = np.concatenate((np.reshape(forcing,(-1,1)), shapes) , axis=1)
  #if (any(shapes == np.inf)):
  #  print("infinite value in U")
  #U = np.nan_to_num(shapes,nan=0.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)
  U = shapes
  model.fit(response,t=t,u=U)
  #model.print()
  #print("score = ",model.score(response,t=t,u=U)) # training data score

  mae = 10**6 # placeholder, shows simulation diverged or wasn't final run
  rmse = 10**6
  simulated = np.ones(shape=(len(response[1:]),1))*np.mean(response)
  if (final_run): 
    '''
    print("number of lines:")
    print(len(shape_factors))
    plt.figure(figsize=(20,10))
    plt.plot(U)
    plt.title("final input transformations")
    plt.show()
    
    plt.figure(figsize=(5,5))
    plt.title("input transformation parameters")
    plt.scatter(loc_factors, shape_factors)
    plt.xlabel("location")
    plt.ylabel("shape")
    plt.show()
    '''

    model.print(precision=5)
    print("score = ",model.score(response,t=t,u=U)) # training data score

    try: # in case simulation diverges
      y = np.reshape(response,(-1,1)) # to match simulation results dimension
      simulated = model.simulate([response[1]],t=t,u=U)
      '''
      fig, ax1 = plt.subplots(figsize=(20,10))
      ax2 = ax1.twinx()
      plt.title(str("polynomial degree = "+ str(poly_degree)))

      line1 = ax1.plot(t[1:],y[1:],'r-.',label='measurements')
      line2 = ax1.plot(t[1:],simulated[:len(t)-1],'g--',label='SINDy')
      ax1.set_xlabel(r'Time ($min$)')
      ax1.set_ylabel(r'response magnitude')
      ax1.set_ylim([1.2*min(y),1.2*max(y)])

      line3 = ax2.plot(t[1:],forcing[1:],'b',label='forcing')
      ax2.set_ylabel("forcing intensity")

      line4 = ax2.plot(t[1:], shapes[1:] * ( np.max(forcing) / np.max(shapes  ) ) , 'k--', label='transformed forcing (scaled)', alpha=0.35)
      
      lns = line1+line2+line3+line4
      labs = [l.get_label() for l in lns]
      plt.legend(lns, labs, loc=0,ncol=3)


      plt.show()
      '''
      mae = np.mean(np.abs(simulated[:len(t)-1]-y[1:])) # mean absolute error
      print("Simulation MAE = ", mae)
      rmse = np.sqrt(np.mean((simulated[:len(t)-1]-y[1:])**2)) # root mean squared error
      print("Simulation RMSE = ", rmse)


    except:
      print("Simulation diverged.")


    


  return [model.score(response,t=t,u=U), model, mae, rmse, t[1:], simulated[:len(t)-1] , response[1:] , forcing[1:] , U]

# takes np array X and assumes the zeroeth column is the forcing
def optimize_lag_shapes(polyorder, target, init_num_lines, max_num_lines, X,max_iter):
  results = list()

  rates_of_change = abs(np.diff(X[:,target]))
  biggest_movers = np.flip(np.argsort(rates_of_change))
  maxes = np.array([biggest_movers[0]])
  if (maxes[0] < 1):
    maxes[0] = 1 # lower bound shape at 1

  # if a close neighbor is already selected, don't want that to be a starting point
  for index in range(0,len(biggest_movers)):
    if ( (abs(biggest_movers[index] - maxes) > int(len(X) / 1000*init_num_lines)).all()   ): 
      # more than some fraction of the total length apart
      # using init_num_lines strikes a balance between starting evenly spaced 
      # and starting all clustered on the steep regions
      if (biggest_movers[index] < 1):
        maxes = np.append(maxes,1) 
      else:
        maxes = np.append(maxes,float(biggest_movers[index]))
    if (len(maxes) >= max_num_lines):
      break

  previous_best = 0

  shape_factors = np.array([])
  scale_factors = np.array([])
  loc_factors = np.array([])
  #speeds =  list([1000,500,200,100,50,10, 5,2, 1.1, 1.05, 1.01, 1.001])
  speeds = list([1000,500,200,100,50,10, 5,2, 1.1, 1.05, 1.01,1.001])
  
  for num_lines in range(init_num_lines,max_num_lines):
    #print(num_lines)
    #speed = 6.4 # how far we'll jump around initially
    speed_idx = 0
    speed = speeds[speed_idx]

    if (len(shape_factors) == 0):  # if we're starting right now
      #shape_factors = maxes[0:init_num_lines]
      # start assuming immediate impact
      shape_factors = np.ones(shape=(init_num_lines,1))
      scale_factors = np.ones(shape = shape_factors.shape)
      loc_factors = np.zeros(shape = shape_factors.shape)

      
    else:
      # start dull
      sharp_cand = maxes[num_lines]
      # start assuming immeidate impact
      #sharp_cand = 1
      delay_cand = 1
      loc_cand = 0

      shape_factors = np.append(shape_factors, sharp_cand)
      scale_factors = np.append(scale_factors, delay_cand)
      loc_factors = np.append(loc_factors, loc_cand)

    # changed prev model to true for verbose output
    prev_model = SINDY_delays_3d(shape_factors, scale_factors, loc_factors, np.arange(0,len(X)), X[:,0], X[:,target],False, polyorder )

    print("\nInitial model:\n")
    print("score")
    print(prev_model[0])
    print("shape factors")
    print(shape_factors)
    print("scale factors")
    print(scale_factors)
    print("location factors")
    print(loc_factors)
    print("")

    for iterations in range(0,max_iter ):
      tuning_line = iterations % num_lines

      sooner_locs = np.array(loc_factors)
      sooner_locs[tuning_line-1] = float(loc_factors[tuning_line-1] - speed*len(X)/10**4  )
      if ( sooner_locs[tuning_line-1] < 0):
        sooner = np.zeros(len(prev_model))
      else:
        sooner = SINDY_delays_3d(shape_factors ,scale_factors ,sooner_locs, 
          np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      
      
      later_locs = np.array(loc_factors)
      later_locs[tuning_line-1] = float ( loc_factors[tuning_line-1]  +   1.01*speed*len(X)/10**4 )
      later = SINDY_delays_3d(shape_factors , scale_factors,later_locs, 
          np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      

      shape_up = np.array(shape_factors)
      shape_up[tuning_line-1] = float ( shape_factors[tuning_line-1]*speed*1.01 )
      shape_upped = SINDY_delays_3d(shape_up , scale_factors, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      
      shape_down = np.array(shape_factors)
      shape_down[tuning_line-1] = float ( shape_factors[tuning_line-1]/speed )
      if (shape_down[tuning_line-1] < 1):
        shape_downed = np.zeros(len(prev_model)) # return a score of zero as this is illegal
      else:
        shape_downed = SINDY_delays_3d(shape_down , scale_factors, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )

      scale_up = np.array(scale_factors)
      scale_up[tuning_line-1] = float(  scale_factors[tuning_line-1]*speed*1.01 )
      scaled_up = SINDY_delays_3d(shape_factors , scale_up, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )


      scale_down = np.array(scale_factors)
      scale_down[tuning_line-1] = float ( scale_factors[tuning_line-1]/speed )
      scaled_down = SINDY_delays_3d(shape_factors , scale_down, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )
      
      # rounder
      rounder_shape = np.array(shape_factors)
      rounder_shape[tuning_line-1] = shape_factors[tuning_line-1]*(speed*1.01)
      rounder_scale = np.array(scale_factors)
      rounder_scale[tuning_line-1] = scale_factors[tuning_line-1]/(speed*1.01)
      rounder = SINDY_delays_3d(rounder_shape , rounder_scale, loc_factors, 
                                np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )

      # sharper
      sharper_shape = np.array(shape_factors)
      sharper_shape[tuning_line-1] = shape_factors[tuning_line-1]/speed
      if (sharper_shape[tuning_line -1] < 1):
        sharper = np.zeros(len(prev_model)) # lower bound on shape to avoid inf
      else:
        sharper_scale = np.array(scale_factors)
        sharper_scale[tuning_line-1] = scale_factors[tuning_line-1]*speed
        sharper = SINDY_delays_3d(sharper_shape ,sharper_scale,loc_factors, 
                                  np.arange(0,len(X)), X[:,0], X[:,target], False, polyorder )


    

      scores = [prev_model[0], shape_upped[0], shape_downed[0], scaled_up[0], scaled_down[0], sooner[0], later[0], rounder[0], sharper[0] ]
      #print(scores)
      if (sooner[0] >= max(scores)):
        prev_model = sooner
        loc_factors = sooner_locs
      elif (later[0] >= max(scores)):
        prev_model = later
        loc_factors = later_locs

      elif(shape_upped[0] >= max(scores)):
        prev_model = shape_upped
        shape_factors = shape_up
      elif(shape_downed[0] >=max(scores)):
        prev_model = shape_downed
        shape_factors = shape_down

      elif(scaled_up[0] >= max(scores)):
        prev_model = scaled_up
        scale_factors = scale_up
      elif(scaled_down[0] >= max(scores)):
        prev_model = scaled_down
        scale_factors = scale_down


      elif (rounder[0] >= max(scores)):
        prev_model = rounder
        shape_factors = np.array(rounder_shape)
        scale_factors = np.array(rounder_scale)
      elif (sharper[0] >= max(scores)):
        prev_model = sharper
        shape_factors = np.array(sharper_shape)
        scale_factors = np.array(sharper_scale)


      elif( (num_lines - 1) == tuning_line): # the middle was best, but it's bad, tighten up the bounds (if we're at the last tuning line)
        
        speed_idx = speed_idx + 1
        
        if (speed_idx >= len(speeds)):
          break
        speed = speeds[speed_idx]
        
        '''
        if (speed > 10):
          speed = speed*0.5 # coarse tuning
          
        elif(speed > 1.3):
          speed = speed*0.8 # fine tuning
        else:
          speed = speed*0.95 # very fine tuning
        '''
        '''
        print("\nprevious, shape up, shape down, scale up, scale down, sooner, later, rounder, sharper")
        print(scores)
        print("speed")
        print(speed)
        print("shape factors")
        print(shape_factors)
        print("scale factors")
        print(scale_factors)
        print("location factors")
        print(loc_factors)
        print("iteration no:")
        print(iterations)
        print("\n")
        '''
        #print(scores)
        if (speed < 1):
          print("converged, max accuracy for number of lines")
          break
    
    
    final_model = SINDY_delays_3d(shape_factors, scale_factors ,loc_factors,np.arange(0,len(X)), X[:,0], X[:,target], True, polyorder )
    results.append([final_model, shape_factors, scale_factors, loc_factors])
    if ((previous_best and final_model[0] - previous_best[0] < 0.005) or ( previous_best and final_model[0] - previous_best[0] < 0.01 and final_model[2] > previous_best[2] )): 
      # marginal (less than half percent improvement) OR somewhat marginal improvement in fit and worse mae
      break # we can be done
    else:
      previous_best = final_model
  winning_model = None
  best_mae = np.Inf
  for num_lines in range(len(results)):
    if (results[num_lines][0][2] < best_mae):
      winning_model = results[num_lines]
      best_mae = results[num_lines][0][2]

  return winning_model
    
def transform_input(shape_factors, scale_factors, loc_factors,t, forcing):
  shape_time = np.arange(0,len(t),1) # analogous to drainage_time
  # shapes is analogous to "drainage" in original code
  feature_names=['response']# , 'forcing']
  shapes = np.zeros(shape=(len(t), len(shape_factors)))
  for shape_idx in range(0,len(shape_factors)):
    for idx in range(0,len(t)):
      if (abs(forcing[idx]) > 10**-6): # when nonzero forcing occurs
        if (idx == int(0)):

          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time, shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 

        else:
          shapes[idx:,shape_idx] = shapes[idx:,shape_idx] + forcing[idx]*stats.gamma.pdf(shape_time[:-idx], shape_factors[shape_idx], scale=scale_factors[shape_idx], loc = loc_factors[shape_idx]) 

  return shapes 




"""# load data"""

print("begin loading training data")
start_index = 0
print("start index")
print(start_index)


with pyswmm.output.Output(filepath_to_trainout) as out:
    # 1200 junctions, so stepping 30 loads about 40 sites

  #depths = pd.DataFrame(columns= list(out.nodes.keys())[start_index:-1:30], index = out.node_series(index=0,attribute=swmm.toolkit.shared_enum.NodeAttribute.INVERT_DEPTH))

  depths = pd.DataFrame(columns= list(["88-55483", "1771_PLYMOUTH_2","37035_6"]), index = out.node_series(index=0,attribute=swmm.toolkit.shared_enum.NodeAttribute.INVERT_DEPTH))



  for col in depths.columns:
    depths[col]= out.node_series(index=col, attribute=swmm.toolkit.shared_enum.NodeAttribute.INVERT_DEPTH).values()

  # all subcatchemnts have the same precip time series so just grab the first subcatchment listed
  depths['precipitation'] = out.subcatch_series(index=0, attribute=swmm.toolkit.shared_enum.SubcatchAttribute.RAINFALL).values()

# i want precipitation to be the first column
cols = depths.columns.tolist()
cols = cols[-1:] + cols[:-1]
depths = depths[cols]
depths.plot(figsize=(25,10))
x = np.array(depths)
#x_test = np.array(depths)

print("training data loaded")
print("begin loading testing data")


with pyswmm.output.Output(filepath_to_testout) as out:
  #test_depths = pd.DataFrame(columns= list(out.nodes.keys())[start_index:-1:30], index = out.node_series(index=0,attribute=swmm.toolkit.shared_enum.NodeAttribute.INVERT_DEPTH))
  test_depths = pd.DataFrame(columns= list(["88-55483", "1771_PLYMOUTH_2","37035_6"]), index = out.node_series(index=0,attribute=swmm.toolkit.shared_enum.NodeAttribute.INVERT_DEPTH))

  for col in test_depths.columns:
    test_depths[col]= out.node_series(index=col, attribute=swmm.toolkit.shared_enum.NodeAttribute.INVERT_DEPTH).values()

  # all subcatchemnts have the same precip time series so just grab the first subcatchment listed
  test_depths['precipitation'] = out.subcatch_series(index=0, attribute=swmm.toolkit.shared_enum.SubcatchAttribute.RAINFALL).values()

# i want precipitation to be the first column
cols = test_depths.columns.tolist()
cols = cols[-1:] + cols[:-1]
test_depths = test_depths[cols]
test_depths.plot(figsize=(25,10))

x_test = np.array(test_depths)
#x = np.array(test_depths)

print("testing data loaded")

"""# run batched evaluation"""

init_num_lines = 1
max_num_lines = 6
max_poly_order = 3
max_iterations = 250 
fake_names = ["Junction A","Junction B", "Junction C"]
fake_name_index = 0
for target in range(1,len(depths.columns)):
  print(str("starting for target "+ str(depths.columns[target])))
  plt.close('all')
  simulations = list()
  mae = list()

  training_simulations = list()
  training_mae = list()
  r2scores=list()


  for polyorder in range(1, max_poly_order+1):
    print(str("order: "+ str(polyorder) ) )
    # train on the training storm (x)
    best_model = optimize_lag_shapes(polyorder, target, init_num_lines, max_num_lines, x , max_iterations) 
    r2scores.append(best_model[0][0])
    # transform the input of the testing storm (x_test)
    U = transform_input(best_model[1], best_model[2], best_model[3], np.arange(len(x_test)) , x_test[:,0]) 
    # simulate the response to the testing storm and compare to actual
    try:
      simulations.append(best_model[0][1].simulate([x_test[0,target]] , np.arange(len(x_test)), u=U))
      mae.append(np.mean(np.abs(simulations[polyorder-1]-x_test[1:,target])))
      print("testing mae")
      print(np.mean(np.abs(simulations[polyorder-1]-x_test[1:,target])))
    except:
      simulations.append(np.ones(len(x_test[1:,target])) * np.mean(x_test[1:,target]) )
      mae.append(10**6)
      print(str("target: " + str( depths.columns[target]) + "  |  polyorder: " + str(polyorder)))
      print("simulation diverged")


    # simulate response to training storm and compare to actual
    U_train = transform_input(best_model[1], best_model[2], best_model[3], np.arange(len(x)) , x[:,0]) 
    try:
        training_simulations.append(best_model[0][1].simulate([x[0,target]] , np.arange(len(x)) , u = U_train ) )
        training_mae.append(np.mean(np.abs(training_simulations[polyorder-1]-x[1:,target])))
        print("training mae")
        print(np.mean(np.abs(training_simulations[polyorder-1]-x[1:,target])))
    except:
        training_simulations.append(np.ones(len(x[1:,target])) * np.mean(x[1:,target]) )
        training_mae.append(10**6)
        print("training simulation diverged")

  # plot the model with the best training R^2 and mae
  # if those measures differ, use the lower order one
  print("r2 scores")
  print(r2scores)
  print("training mae")
  print(training_mae)
  print("testing mae")
  print(mae)

  best_r2 = np.argmax(r2scores)
  best_mae = np.argmin(training_mae)
  print("best r2 index")
  print(best_r2)
  print("best mae index")
  print(best_mae)
  # plot the model that had the best training mae
  to_plot_idx = best_mae #min(best_r2, best_mae) 

  print("plotting model: ")
  print(to_plot_idx)




  # visualize training
  fig = plt.figure(figsize=(25,10))
  gs = GridSpec(2,1, figure=fig, height_ratios=[1,4])
  
  
  train_days = np.arange(0,len(x[1:,target]))/24/60


  ax2 = fig.add_subplot(gs[0,0])
  #ax2.set_ylim([0,max( x[1:,0])])
  ax2.invert_yaxis()
  ax2.tick_params(axis='both',labelsize='xx-large')
  ax2.tick_params(axis='x',labelbottom=False,which='both',bottom=False)
  l5 = ax2.plot(train_days,x[1:,0], 'b',label='Rainfall',alpha=1,linewidth=5)

  ax2.legend(["Rainfall [in/hr]"],fontsize=45,loc=0)
  ax2.spines['right'].set_visible(False)
  ax2.spines['bottom'].set_visible(False)

  ax1 = fig.add_subplot(gs[1,0])
  ax1.tick_params(axis='both',labelsize='xx-large')
  l1 = ax1.plot(train_days,x[1:,target],'k',label='EPA-SWMM',alpha=0.5,linewidth=10)
  l2 = ax1.plot(train_days,training_simulations[to_plot_idx],'r--',linewidth=5,label=str("Best Fit"),alpha=1)

  ax1.set_xlabel(r'Time [days]',fontsize=35)
  ax1.set_ylabel(r'Depth [ft]',fontsize=35)
  #ax1.set_ylim([min(0,min(x[1:,target])), max( max(x[1:,target]   ) , max(training_simulations[to_plot_idx]) )  ]    )
  ax1.spines['top'].set_visible(False)
  ax1.spines['right'].set_visible(False)


  lns = l1+l2
  labs = [l.get_label() for l in lns]
  ax1.legend(lns, labs, loc=0,fontsize=45)
  #fig.suptitle(str("Training "+ str(depths.columns[target]  + " | R^2 = {r2:.2f}".format(r2=r2scores[to_plot_idx])  )  ) ,fontsize=45,y=0.98)
  fig.suptitle(str("Training "+ str(fake_names[fake_name_index]  + " | R^2 = {r2:.2f}".format(r2=r2scores[to_plot_idx])  )  ) ,fontsize=45,y=0.98)

  plt.tight_layout()
  plt.savefig(str("G:/My Drive/SINDy/SWMM-model-reduction/AnnArborFull/swmm_results/target" + str(test_depths.columns.to_list()[target]) + "_training_results.png"), dpi=300)
  plt.close()
        
        

  # visualize testing
  fig  = plt.figure(figsize=(25,10))
  gs = GridSpec(2,1, figure=fig, height_ratios=[1,4])
  
  
  test_days = np.arange(0,len(x_test[1:,target]))/24/60

  ax2 = fig.add_subplot(gs[0,0])

  ax2.invert_yaxis()
  ax2.tick_params(axis='both',labelsize='xx-large')
  ax2.tick_params(axis='x',labelbottom=False,which='both',bottom=False)
  l5 = ax2.plot(test_days,x_test[1:,0], 'b',label='Rainfall',alpha=1,linewidth=5)
  ax2.legend(["Rainfall [in/hr]"],fontsize=45,loc=0)
  ax2.spines['right'].set_visible(False)
  ax2.spines['bottom'].set_visible(False)

  ax1 = fig.add_subplot(gs[1,0])
  ax1.tick_params(axis='both',labelsize='xx-large')
  l1 = ax1.plot(test_days,x_test[1:,target],'k',label='EPA-SWMM',alpha=0.5,linewidth=10)
  l2 = ax1.plot(test_days,simulations[to_plot_idx],'g--',label=str("Prediction"),alpha=1,linewidth=5)

  ax1.set_xlabel(r'Time [days]',fontsize=35)
  ax1.set_ylabel(r'Depth [ft]',fontsize=35)
  #ax1.set_ylim([min(0,min(x_test[1:,target])), max( max(x_test[1:,target]) , max(simulations[to_plot_idx]) )  ] )
  ax1.spines['top'].set_visible(False)
  ax1.spines['right'].set_visible(False)

  lns = l1+l2

  labs = [l.get_label() for l in lns]
  ax1.legend(lns, labs, loc=0,fontsize=45)
  #fig.suptitle(str("Testing "+ str(depths.columns[target]) + " | MAE = {mae:.2f} ft".format(mae=mae[to_plot_idx]) ),fontsize=45,y=0.98)
  fig.suptitle(str("Testing "+ str(fake_names[fake_name_index]  + " | MAE = {mae:.2f} ft".format(mae=mae[to_plot_idx]) ) ) ,fontsize=45,y=0.98)
  plt.tight_layout()
  plt.savefig(str("G:/My Drive/SINDy/SWMM-model-reduction/AnnArborFull/swmm_results/target" + str(test_depths.columns.to_list()[target]) + "_testgen_results.png"), dpi=300)
  plt.close()

  fake_name_index = fake_name_index +1

  #plt.show()
